# BLIP2-Japanese

The model has been trained for stage1 using COCO dataset with [STAIR captions](http://captions.stair.center/#:~:text=STAIR%20Captions%20is%20a%20large,multimodal%20retrieval%2C%20and%20image%20generation.).

Sample captions generated by the model on COCO images can be found in playground.ipynb.

## Use Case: Generate Japanese Captions for Captioning Datasets

The weights of Blip2_Japanese_qformer trained on STAIR can be obtained from [this link](https://drive.google.com/drive/folders/11YRyQb-_Pn8g3Wlnv2aBwNnvZ0Oo4LRM?usp=drive_link).

Copy the whole folder under lavis directory to run the example jupyter notebook.

Captions generated for [flickr30k dataset](https://www.kaggle.com/datasets/adityajn105/flickr30k?select=Images) can be found in flicker30k_caption.json.
